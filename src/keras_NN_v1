#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 16 19:53:00 2016

@author: heslote1
"""

  # Assumption: if you are not seizing you are pre-seize
        
        # Prepare for Neural Network
        # Take last 10 seconds
        # Vectorize matrix 10*23 points
        # Predict
        
#import progressbar        
#from time import sleep

# LSTM for international airline passengers problem with regression framing
import numpy as np
import scipy as sp 
import os
import matplotlib.pyplot as plt
import pandas
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.optimizers import SGD
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# convert an array of values into a dataset matrix
look_back = 10 # 10 seconds of data required
look_fwd  = 30 # 30 seconds into the future
s_period  = 1  # 1 second per sample 

def create_NN_data(data):
    dataX, dataY = [], []
    for j in range(data['seiz'][0].shape[0]-look_back):  
        dataX.append( np.array([np.concatenate((
            data['freq'][0][j:(j+look_back), :].flatten(),
            data['freq'][1][j:(j+look_back), :].flatten(),
            data['amp' ][0][j:(j+look_back), :].flatten(),
            data['amp' ][1][j:(j+look_back), :].flatten())).tolist()])
            )
        dataY.append(np.array([[
            data['seiz'][0][j,1],
            data['seiz'][1][j,1],
            data['seiz'][2][j,1]]]).astype(float)
            )

    return dataX, dataY
 
# https://github.com/fchollet/keras/issues/369
def halfsse(y_true, y_pred):
    '''1/2*Sum_Squared_Error'''
    from keras import backend as K
    E = K.square(y_pred - y_true)/2
    return E
    
# fix random seed for reproducibility
np.random.seed(7)

root  = "/Users/heslote1/www.physionet.org/pn6/chbmit/timeseries/" # end all folders with "/"
mfiles = [x for x in os.listdir(root)][1:-1]
createModel = True
for mfile in mfiles:
    filename = os.path.join(root,mfile)
    # load the dataset
    data            = sp.io.loadmat(filename)
    dataX, dataY    = create_NN_data(data)    
    
    # split into train and test sets
    size_test  = int(len(dataX) * 0.67)
    #size_train = len(dataX) - size_test

    np.random.seed(7) # fix random seed for reproducibility
    test  = np.random.choice(len(dataX), size_test)
    train = np.setdiff1d(range(len(dataX)), test) 
    
    x_train, y_train  = [dataX[i] for i in train], [dataY[i] for i in train]
    x_test,  y_test   = [dataX[i] for i in test],  [dataY[i] for i in test]
    
    nb_inputs = len(dataX[0][0])
    # reshape input to be [samples, time steps, features]
    #x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
    #x_test  = np.reshape(x_test,  (x_test.shape[0],  1, x_test.shape[1]))
    
    if createModel:
        # create model
        model = Sequential()
        np.random.seed(7) # fix random seed for reproducibility
        model.add(Dense(1000, input_dim=nb_inputs, activation='sigmoid', bias=False)) # Layer 1
        np.random.seed(7) # fix random seed for reproducibility
        model.add(Dense(50, activation='sigmoid', bias=False))             # Layer 2
        np.random.seed(7) # fix random seed for reproducibility
        model.add(Dense(10, activation='sigmoid', bias=False))             # Layer 3
        np.random.seed(7) # fix random seed for reproducibility
        model.add(Dense(3, activation='sigmoid', bias=False))             # Layer 3
        
        # Compile model
        eta = 1
        sgd = SGD(lr=eta, momentum=0.0, decay=0.0, nesterov=False)    
        model.compile(loss=halfsse, optimizer=sgd, metrics=['accuracy'])
        createModel = False
    
    # Train the model for a fixed number of epochs
    epochs = 3
    ok = True
    for i in range(len(x_train)):
        if dataY[i].any():
            ok = not ok
        
        if use:
            model.fit(x_train[i], y_train[i], batch_size=1, nb_epoch=epochs, verbose=1, callbacks=[], 
                  validation_split=0.0, validation_data=None, shuffle=False, 
                  class_weight=None, sample_weight=None)
            ok = not ok
            
    for i in range(len(dataY)):
#        print dataY[i].any()
        if y_train[i].any():
            break
    
#    for layer in model.layers:
#        print(layer.get_weights())
    
#    # Computes the loss on some input data, batch by batch.
#    for i in range(len(x_train)):
#        model.evaluate(x_train[i], y_train[i], batch_size=1, verbose=2, sample_weight=None)

# make predictions
predict_train = model.predict(x_train[i])
predict_test  = model.predict(x_test[i])
    
## calculate root mean squared error
#score_train = math.sqrt(mean_squared_error(y_train, predict_train))
#print('Train Score: %.2f RMSE' % (score_train))
#score_test = math.sqrt(mean_squared_error(testY[0], predict_test[:,0]))
#print('Test Score: %.2f RMSE' % (score_test))
## shift train predictions for plotting
#trainPredictPlot = numpy.empty_like(dataset)
#trainPredictPlot[:, :] = numpy.nan
#trainPredictPlot[look_back:len(predict_train)+look_back, :] = predict_train
## shift test predictions for plotting
#testPredictPlot = numpy.empty_like(dataset)
#testPredictPlot[:, :] = numpy.nan
#testPredictPlot[len(predict_train)+(look_back*2)+1:len(dataset)-1, :] = predict_test
## plot baseline and predictions
#plt.plot(scaler.inverse_transform(dataset))
#plt.plot(trainPredictPlot)
#plt.plot(testPredictPlot)
#plt.show()